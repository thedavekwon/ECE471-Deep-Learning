{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "\n",
    "from torch.utils import data\n",
    "from torchsummary import summary\n",
    "from ResNet import *\n",
    "from radam import RAdam\n",
    "\n",
    "\n",
    "def load_CIFAR10_data():\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ]\n",
    "    )\n",
    "    train_set = torchvision.datasets.CIFAR10(root='./dataset', train=True,\n",
    "                                             download=True, transform=train_transform)\n",
    "    train_set, validation_set = data.random_split(train_set,\n",
    "                                                  (int(len(train_set) * 0.9), int(len(train_set) * 0.1)))\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./dataset', train=False,\n",
    "                                            download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = data.DataLoader(train_set, batch_size=2500,\n",
    "                                   shuffle=True, num_workers=0, pin_memory=True)\n",
    "    validation_loader = data.DataLoader(validation_set, batch_size=2500,\n",
    "                                        shuffle=True, num_workers=0, pin_memory=True)\n",
    "    test_loader = data.DataLoader(test_set, batch_size=2500,\n",
    "                                  shuffle=False, num_workers=0, pin_memory=True)\n",
    "    return train_loader, validation_loader, test_loader, test_set.classes\n",
    "\n",
    "\n",
    "def load_CIFAR100_data():\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ]\n",
    "    )\n",
    "    train_set = torchvision.datasets.CIFAR100(root='./dataset', train=True,\n",
    "                                              download=True, transform=transform)\n",
    "    train_set, validation_set = data.random_split(train_set, (int(len(train_set) * 0.9), int(len(train_set) * 0.1)))\n",
    "    test_set = torchvision.datasets.CIFAR100(root='./dataset', train=False,\n",
    "                                             download=True, transform=transform)\n",
    "\n",
    "    train_loader = data.DataLoader(train_set, batch_size=2500,\n",
    "                                   shuffle=True, num_workers=0,\n",
    "                                   pin_memory=True)\n",
    "    validation_loader = data.DataLoader(validation_set, batch_size=2500,\n",
    "                                        shuffle=True, num_workers=0,\n",
    "                                        pin_memory=True)\n",
    "    test_loader = data.DataLoader(test_set, batch_size=2500,\n",
    "                                  shuffle=True, num_workers=0,\n",
    "                                  pin_memory=True)\n",
    "\n",
    "    return train_loader, validation_loader, test_loader, test_set.classes\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def accuracy_plot(accuracies, TEST):\n",
    "    plt.plot(range(1, len(accuracies) + 1), accuracies)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\\n(%)\").set_rotation(0)\n",
    "\n",
    "    if (TEST):\n",
    "        plt.legend([\"train\", \"test\"])\n",
    "    else:\n",
    "        plt.legend([\"train\", \"validation\"])\n",
    "\n",
    "\n",
    "def losses_plot(train_losses, losses, TEST):\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses)\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\").set_rotation(0)\n",
    "\n",
    "    if (TEST):\n",
    "        plt.legend([\"train\", \"test\"])\n",
    "    else:\n",
    "        plt.legend([\"train\", \"validation\"])\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device, epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for X, y in train_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        regularization_loss = 0.0\n",
    "        for param in model.parameters():\n",
    "            regularization_loss += torch.norm(param)\n",
    "        loss = criterion(outputs, y) + LAMBDA * regularization_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validate(model, validation_loader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    vali_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in validation_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(X)\n",
    "            vali_loss += criterion(outputs, y).item()\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    vali_loss /= len(validation_loader.dataset)\n",
    "    vali_percentage = round(correct / len(validation_loader.dataset) * 100, 2)\n",
    "    print(\n",
    "        f\"Epoch:{epoch} Validation loss: {vali_loss:0.6f}, Validation Accuracy:{correct}/{len(validation_loader.dataset)} ({vali_percentage}%)\"\n",
    "    )\n",
    "    return vali_percentage, vali_loss\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device, epoch, classes, topk):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(X)\n",
    "            test_loss += criterion(outputs, y).item()\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            if (topk == 1):\n",
    "                pred = outputs.argmax(dim=1, keepdim=True)\n",
    "                correct_pred = pred.eq(y.view_as(pred))\n",
    "                correct += correct_pred.sum().item()\n",
    "            else:\n",
    "                _, pred = outputs.topk(topk, 1, True, True)\n",
    "                correct_pred = pred.eq(y.view(-1, 1).expand_as(pred))\n",
    "                correct += correct_pred.sum().item()\n",
    "            \n",
    "    test_percentage = round(correct / len(test_loader.dataset) * 100, 2)\n",
    "    print(\n",
    "        f\"Epoch:{epoch} Test loss: {test_loss:0.6f}, Test Accuracy:{correct}/{len(test_loader.dataset)} ({test_percentage}%)\"\n",
    "    )\n",
    "    return test_percentage, test_loss\n",
    "\n",
    "\n",
    "class convLayer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, pooling_size, padding_size, dropout_rate):\n",
    "        super(convLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding_size)\n",
    "        self.pooling_size = pooling_size\n",
    "        if self.pooling_size:\n",
    "            self.pool = nn.MaxPool2d(pooling_size)\n",
    "        self.bm = nn.BatchNorm2d(out_channel)\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bm(x)\n",
    "        if self.pooling_size:\n",
    "            x = self.pool(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# https://discuss.pytorch.org/t/flatten-layer-of-pytorch-build-by-sequential-container/5983\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class CIFAR10ResModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10ResModel, self).__init__()\n",
    "        self.res = resnet(3, 2048)\n",
    "        self.fc1 = nn.Linear(2048, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 10)\n",
    "        self.layers = [ self.res, self.fc1, self.fc2, self.fc3]\n",
    "        self.activations = [True, True, True, True]\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            if activation:\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class CIFAR10ResLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10ResLayer, self).__init__()\n",
    "        self.conv1 = convLayer(3, 64, 3, 2, 1, 0.3)\n",
    "        self.conv2 = convLayer(64, 128, 3, 2, 1, 0.3)\n",
    "        self.conv3 = convLayer(128, 256, 5, 2, 1, 0.3)\n",
    "        self.res1 = ResNetLayer(256, 512, n=3)\n",
    "        self.conv4 = convLayer(512, 512, 2, 2, 1, 0.4)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = nn.Linear(8192, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 10)\n",
    "        self.layers = [self.conv1, self.conv2, self.conv3, self.res1, self.flatten, self.fc1, self.fc2, self.fc3]\n",
    "        self.activations = [False, False, False, False, False, True, True, True]\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            if activation:\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class CIFAR10Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10Model, self).__init__()\n",
    "        self.conv1 = convLayer(3, 64, 3, 2, 1, 0.3)\n",
    "        self.conv2 = convLayer(64, 128, 3, 0, 1, 0.3)\n",
    "        self.conv3 = convLayer(128, 256, 5, 2, 1, 0.3)\n",
    "        self.conv4 = convLayer(256, 512, 5, 2, 1, 0.3)\n",
    "        self.conv5 = convLayer(512, 512, 3, 0, 1, 0.4)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = nn.Linear(2048, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 10)\n",
    "        self.layers = [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.flatten, self.fc1, self.fc2, self.fc3]\n",
    "        self.activations = [False, False, False, False, False, False, True, True, True]\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            if activation:\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "class CIFAR10ModelTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10ModelTest, self).__init__()\n",
    "        self.conv1 = convLayer(3, 64, 3, 2, 1, 0.3)\n",
    "        self.conv2 = convLayer(64, 128, 3, 0, 1, 0.3)\n",
    "        self.conv3 = convLayer(128, 256, 5, 2, 1, 0.3)\n",
    "        self.conv4 = convLayer(256, 512, 5, 0, 1, 0.3)\n",
    "        self.conv5 = convLayer(512, 512, 1, 2, 1, 0.4)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = nn.Linear(4608, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 10)\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        self.layers = [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.flatten, self.fc1, self.fc2, self.fc3, self.fc4]\n",
    "        self.activations = [False, False, False, False, False, False, True, True, True, True]\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            if activation:\n",
    "                x = self.drop(F.relu(layer(x)))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "class CIFAR100Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR100Model, self).__init__()\n",
    "        self.conv1 = convLayer(3, 64, 3, 2, 1, 0.4)\n",
    "        self.conv2 = convLayer(64, 128, 3, 0, 1, 0.4)\n",
    "        self.conv3 = convLayer(128, 256, 5, 2, 2, 0.4)\n",
    "        self.conv4 = convLayer(256, 512, 5, 2, 2, 0.4)\n",
    "        self.conv5 = convLayer(512, 512, 1, 0, 0, 0.4)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = nn.Linear(8192, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 100)\n",
    "        self.layers = [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.flatten, self.fc1, self.fc2, self.fc3]\n",
    "        self.activations = [False, False, False, False, False, False, True, True, True]\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            if activation:\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "def transfer_radam_densenet(num_classes):\n",
    "    model = torchvision.models.densenet161(pretrained=True)\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = False\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_features, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(1024, num_classes),\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = RAdam(model.fc.parameters())\n",
    "    return model, criterion, optimizer    \n",
    "    \n",
    "def transfer_radam_shufflenet(num_classes):\n",
    "    model = torchvision.models.shufflenet_v2_x1_0(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_features = model.fc.in_features\n",
    "    print(num_features)\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_features, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, num_classes),\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = RAdam(model.fc.parameters())\n",
    "    return model, criterion, optimizer    \n",
    "\n",
    "def transfer_radam_resnet101(num_classes):\n",
    "    model = torchvision.models.resnet101(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_features, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(1024, num_classes),\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = RAdam(model.fc.parameters())\n",
    "    return model, criterion, optimizer    \n",
    "\n",
    "def transfer_learning_resnet152(num_classes):\n",
    "    model = torchvision.models.resnet152(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_features, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(1024, num_classes),\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = RAdam(model.fc.parameters())\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, validation_loader, test_loader, classes = load_CIFAR100_data()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5            [-1, 256, 8, 8]          16,384\n",
      "       BatchNorm2d-6            [-1, 256, 8, 8]             512\n",
      "        Conv2dAuto-7             [-1, 64, 8, 8]           4,096\n",
      "       BatchNorm2d-8             [-1, 64, 8, 8]             128\n",
      "              ReLU-9             [-1, 64, 8, 8]               0\n",
      "       Conv2dAuto-10             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-11             [-1, 64, 8, 8]             128\n",
      "             ReLU-12             [-1, 64, 8, 8]               0\n",
      "       Conv2dAuto-13            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
      "             ReLU-15            [-1, 256, 8, 8]               0\n",
      "ResNetBottleNeckBlock-16            [-1, 256, 8, 8]               0\n",
      "       Conv2dAuto-17             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-18             [-1, 64, 8, 8]             128\n",
      "             ReLU-19             [-1, 64, 8, 8]               0\n",
      "       Conv2dAuto-20             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-21             [-1, 64, 8, 8]             128\n",
      "             ReLU-22             [-1, 64, 8, 8]               0\n",
      "       Conv2dAuto-23            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
      "             ReLU-25            [-1, 256, 8, 8]               0\n",
      "ResNetBottleNeckBlock-26            [-1, 256, 8, 8]               0\n",
      "       Conv2dAuto-27             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
      "             ReLU-29             [-1, 64, 8, 8]               0\n",
      "       Conv2dAuto-30             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-31             [-1, 64, 8, 8]             128\n",
      "             ReLU-32             [-1, 64, 8, 8]               0\n",
      "       Conv2dAuto-33            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
      "             ReLU-35            [-1, 256, 8, 8]               0\n",
      "ResNetBottleNeckBlock-36            [-1, 256, 8, 8]               0\n",
      "      ResNetLayer-37            [-1, 256, 8, 8]               0\n",
      "           Conv2d-38            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-39            [-1, 512, 4, 4]           1,024\n",
      "       Conv2dAuto-40            [-1, 128, 8, 8]          32,768\n",
      "      BatchNorm2d-41            [-1, 128, 8, 8]             256\n",
      "             ReLU-42            [-1, 128, 8, 8]               0\n",
      "       Conv2dAuto-43            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-44            [-1, 128, 4, 4]             256\n",
      "             ReLU-45            [-1, 128, 4, 4]               0\n",
      "       Conv2dAuto-46            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-47            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-48            [-1, 512, 4, 4]               0\n",
      "ResNetBottleNeckBlock-49            [-1, 512, 4, 4]               0\n",
      "       Conv2dAuto-50            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-51            [-1, 128, 4, 4]             256\n",
      "             ReLU-52            [-1, 128, 4, 4]               0\n",
      "       Conv2dAuto-53            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-54            [-1, 128, 4, 4]             256\n",
      "             ReLU-55            [-1, 128, 4, 4]               0\n",
      "       Conv2dAuto-56            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-57            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-58            [-1, 512, 4, 4]               0\n",
      "ResNetBottleNeckBlock-59            [-1, 512, 4, 4]               0\n",
      "       Conv2dAuto-60            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-61            [-1, 128, 4, 4]             256\n",
      "             ReLU-62            [-1, 128, 4, 4]               0\n",
      "       Conv2dAuto-63            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-64            [-1, 128, 4, 4]             256\n",
      "             ReLU-65            [-1, 128, 4, 4]               0\n",
      "       Conv2dAuto-66            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-67            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-68            [-1, 512, 4, 4]               0\n",
      "ResNetBottleNeckBlock-69            [-1, 512, 4, 4]               0\n",
      "       Conv2dAuto-70            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-71            [-1, 128, 4, 4]             256\n",
      "             ReLU-72            [-1, 128, 4, 4]               0\n",
      "       Conv2dAuto-73            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-74            [-1, 128, 4, 4]             256\n",
      "             ReLU-75            [-1, 128, 4, 4]               0\n",
      "       Conv2dAuto-76            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-77            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-78            [-1, 512, 4, 4]               0\n",
      "ResNetBottleNeckBlock-79            [-1, 512, 4, 4]               0\n",
      "      ResNetLayer-80            [-1, 512, 4, 4]               0\n",
      "           Conv2d-81           [-1, 1024, 2, 2]         524,288\n",
      "      BatchNorm2d-82           [-1, 1024, 2, 2]           2,048\n",
      "       Conv2dAuto-83            [-1, 256, 4, 4]         131,072\n",
      "      BatchNorm2d-84            [-1, 256, 4, 4]             512\n",
      "             ReLU-85            [-1, 256, 4, 4]               0\n",
      "       Conv2dAuto-86            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-87            [-1, 256, 2, 2]             512\n",
      "             ReLU-88            [-1, 256, 2, 2]               0\n",
      "       Conv2dAuto-89           [-1, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-90           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-91           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-92           [-1, 1024, 2, 2]               0\n",
      "       Conv2dAuto-93            [-1, 256, 2, 2]         262,144\n",
      "      BatchNorm2d-94            [-1, 256, 2, 2]             512\n",
      "             ReLU-95            [-1, 256, 2, 2]               0\n",
      "       Conv2dAuto-96            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-97            [-1, 256, 2, 2]             512\n",
      "             ReLU-98            [-1, 256, 2, 2]               0\n",
      "       Conv2dAuto-99           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-100           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-101           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-102           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-103            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-104            [-1, 256, 2, 2]             512\n",
      "            ReLU-105            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-106            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-107            [-1, 256, 2, 2]             512\n",
      "            ReLU-108            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-109           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-110           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-111           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-112           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-113            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-114            [-1, 256, 2, 2]             512\n",
      "            ReLU-115            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-116            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-117            [-1, 256, 2, 2]             512\n",
      "            ReLU-118            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-119           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-120           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-121           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-122           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-123            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-124            [-1, 256, 2, 2]             512\n",
      "            ReLU-125            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-126            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-127            [-1, 256, 2, 2]             512\n",
      "            ReLU-128            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-129           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-130           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-131           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-132           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-133            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-134            [-1, 256, 2, 2]             512\n",
      "            ReLU-135            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-136            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-137            [-1, 256, 2, 2]             512\n",
      "            ReLU-138            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-139           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-140           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-141           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-142           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-143            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-144            [-1, 256, 2, 2]             512\n",
      "            ReLU-145            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-146            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-147            [-1, 256, 2, 2]             512\n",
      "            ReLU-148            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-149           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-150           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-151           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-152           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-153            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-154            [-1, 256, 2, 2]             512\n",
      "            ReLU-155            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-156            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-157            [-1, 256, 2, 2]             512\n",
      "            ReLU-158            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-159           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-160           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-161           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-162           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-163            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-164            [-1, 256, 2, 2]             512\n",
      "            ReLU-165            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-166            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-167            [-1, 256, 2, 2]             512\n",
      "            ReLU-168            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-169           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-170           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-171           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-172           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-173            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-174            [-1, 256, 2, 2]             512\n",
      "            ReLU-175            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-176            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-177            [-1, 256, 2, 2]             512\n",
      "            ReLU-178            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-179           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-180           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-181           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-182           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-183            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-184            [-1, 256, 2, 2]             512\n",
      "            ReLU-185            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-186            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-187            [-1, 256, 2, 2]             512\n",
      "            ReLU-188            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-189           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-190           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-191           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-192           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-193            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-194            [-1, 256, 2, 2]             512\n",
      "            ReLU-195            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-196            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-197            [-1, 256, 2, 2]             512\n",
      "            ReLU-198            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-199           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-200           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-201           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-202           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-203            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-204            [-1, 256, 2, 2]             512\n",
      "            ReLU-205            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-206            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-207            [-1, 256, 2, 2]             512\n",
      "            ReLU-208            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-209           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-210           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-211           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-212           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-213            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-214            [-1, 256, 2, 2]             512\n",
      "            ReLU-215            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-216            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-217            [-1, 256, 2, 2]             512\n",
      "            ReLU-218            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-219           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-220           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-221           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-222           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-223            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-224            [-1, 256, 2, 2]             512\n",
      "            ReLU-225            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-226            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-227            [-1, 256, 2, 2]             512\n",
      "            ReLU-228            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-229           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-230           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-231           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-232           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-233            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-234            [-1, 256, 2, 2]             512\n",
      "            ReLU-235            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-236            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-237            [-1, 256, 2, 2]             512\n",
      "            ReLU-238            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-239           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-240           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-241           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-242           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-243            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-244            [-1, 256, 2, 2]             512\n",
      "            ReLU-245            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-246            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-247            [-1, 256, 2, 2]             512\n",
      "            ReLU-248            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-249           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-250           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-251           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-252           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-253            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-254            [-1, 256, 2, 2]             512\n",
      "            ReLU-255            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-256            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-257            [-1, 256, 2, 2]             512\n",
      "            ReLU-258            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-259           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-260           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-261           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-262           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-263            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-264            [-1, 256, 2, 2]             512\n",
      "            ReLU-265            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-266            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-267            [-1, 256, 2, 2]             512\n",
      "            ReLU-268            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-269           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-270           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-271           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-272           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-273            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-274            [-1, 256, 2, 2]             512\n",
      "            ReLU-275            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-276            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-277            [-1, 256, 2, 2]             512\n",
      "            ReLU-278            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-279           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-280           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-281           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-282           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-283            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-284            [-1, 256, 2, 2]             512\n",
      "            ReLU-285            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-286            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-287            [-1, 256, 2, 2]             512\n",
      "            ReLU-288            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-289           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-290           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-291           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-292           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-293            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-294            [-1, 256, 2, 2]             512\n",
      "            ReLU-295            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-296            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-297            [-1, 256, 2, 2]             512\n",
      "            ReLU-298            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-299           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-300           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-301           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-302           [-1, 1024, 2, 2]               0\n",
      "      Conv2dAuto-303            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-304            [-1, 256, 2, 2]             512\n",
      "            ReLU-305            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-306            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-307            [-1, 256, 2, 2]             512\n",
      "            ReLU-308            [-1, 256, 2, 2]               0\n",
      "      Conv2dAuto-309           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-310           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-311           [-1, 1024, 2, 2]               0\n",
      "ResNetBottleNeckBlock-312           [-1, 1024, 2, 2]               0\n",
      "     ResNetLayer-313           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-314           [-1, 2048, 1, 1]       2,097,152\n",
      "     BatchNorm2d-315           [-1, 2048, 1, 1]           4,096\n",
      "      Conv2dAuto-316            [-1, 512, 2, 2]         524,288\n",
      "     BatchNorm2d-317            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-318            [-1, 512, 2, 2]               0\n",
      "      Conv2dAuto-319            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-320            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-321            [-1, 512, 1, 1]               0\n",
      "      Conv2dAuto-322           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-323           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-324           [-1, 2048, 1, 1]               0\n",
      "ResNetBottleNeckBlock-325           [-1, 2048, 1, 1]               0\n",
      "      Conv2dAuto-326            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-327            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-328            [-1, 512, 1, 1]               0\n",
      "      Conv2dAuto-329            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-330            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-331            [-1, 512, 1, 1]               0\n",
      "      Conv2dAuto-332           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-333           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-334           [-1, 2048, 1, 1]               0\n",
      "ResNetBottleNeckBlock-335           [-1, 2048, 1, 1]               0\n",
      "      Conv2dAuto-336            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-337            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-338            [-1, 512, 1, 1]               0\n",
      "      Conv2dAuto-339            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-340            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-341            [-1, 512, 1, 1]               0\n",
      "      Conv2dAuto-342           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-343           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-344           [-1, 2048, 1, 1]               0\n",
      "ResNetBottleNeckBlock-345           [-1, 2048, 1, 1]               0\n",
      "     ResNetLayer-346           [-1, 2048, 1, 1]               0\n",
      "   ResNetEncoder-347           [-1, 2048, 1, 1]               0\n",
      "AdaptiveAvgPool2d-348           [-1, 2048, 1, 1]               0\n",
      "          Linear-349                  [-1, 100]         204,900\n",
      "   ResNetDecoder-350                  [-1, 100]               0\n",
      "================================================================\n",
      "Total params: 42,705,060\n",
      "Trainable params: 42,705,060\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 9.04\n",
      "Params size (MB): 162.91\n",
      "Estimated Total Size (MB): 171.96\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Test loss: 13028.691895, Test Accuracy:254/5000 (5.08%)\n",
      "Epoch:2 Test loss: 14.158797, Test Accuracy:250/5000 (5.0%)\n",
      "Epoch:3 Test loss: 922.481201, Test Accuracy:232/5000 (4.64%)\n",
      "Epoch:4 Test loss: 9.221311, Test Accuracy:326/5000 (6.52%)\n",
      "Epoch:5 Test loss: 9.094468, Test Accuracy:357/5000 (7.14%)\n",
      "Epoch:6 Test loss: 9.064943, Test Accuracy:441/5000 (8.82%)\n",
      "Epoch:7 Test loss: 8.915823, Test Accuracy:516/5000 (10.32%)\n",
      "Epoch:8 Test loss: 8.539066, Test Accuracy:780/5000 (15.6%)\n",
      "Epoch:9 Test loss: 9.217636, Test Accuracy:899/5000 (17.98%)\n",
      "Epoch:10 Test loss: 8.832684, Test Accuracy:725/5000 (14.5%)\n",
      "Epoch:11 Test loss: 8.082374, Test Accuracy:1202/5000 (24.04%)\n",
      "Epoch:12 Test loss: 8.181711, Test Accuracy:1128/5000 (22.56%)\n",
      "Epoch:13 Test loss: 7.679674, Test Accuracy:1544/5000 (30.88%)\n",
      "Epoch:14 Test loss: 7.469842, Test Accuracy:1679/5000 (33.58%)\n",
      "Epoch:15 Test loss: 7.804352, Test Accuracy:1415/5000 (28.3%)\n",
      "Epoch:16 Test loss: 7.189603, Test Accuracy:1947/5000 (38.94%)\n",
      "Epoch:17 Test loss: 7.271515, Test Accuracy:1852/5000 (37.04%)\n",
      "Epoch:18 Test loss: 6.970140, Test Accuracy:2117/5000 (42.34%)\n",
      "Epoch:19 Test loss: 6.825740, Test Accuracy:2230/5000 (44.6%)\n",
      "Epoch:20 Test loss: 6.876210, Test Accuracy:2201/5000 (44.02%)\n",
      "Epoch:21 Test loss: 6.733240, Test Accuracy:2286/5000 (45.72%)\n",
      "Epoch:22 Test loss: 6.765001, Test Accuracy:2328/5000 (46.56%)\n",
      "Epoch:23 Test loss: 6.766563, Test Accuracy:2322/5000 (46.44%)\n",
      "Epoch:24 Test loss: 6.600570, Test Accuracy:2380/5000 (47.6%)\n",
      "Epoch:25 Test loss: 6.702053, Test Accuracy:2406/5000 (48.12%)\n",
      "Epoch:26 Test loss: 6.533957, Test Accuracy:2530/5000 (50.6%)\n",
      "Epoch:27 Test loss: 6.796190, Test Accuracy:2404/5000 (48.08%)\n",
      "Epoch:28 Test loss: 6.692485, Test Accuracy:2512/5000 (50.24%)\n",
      "Epoch:29 Test loss: 6.963769, Test Accuracy:2449/5000 (48.98%)\n",
      "Epoch:30 Test loss: 6.983212, Test Accuracy:2488/5000 (49.76%)\n",
      "Epoch:31 Test loss: 7.917715, Test Accuracy:2265/5000 (45.3%)\n",
      "Epoch:32 Test loss: 7.111501, Test Accuracy:2608/5000 (52.16%)\n",
      "Epoch:33 Test loss: 7.427109, Test Accuracy:2528/5000 (50.56%)\n",
      "Epoch:34 Test loss: 7.485415, Test Accuracy:2562/5000 (51.24%)\n",
      "Epoch:35 Test loss: 7.761441, Test Accuracy:2526/5000 (50.52%)\n",
      "Epoch:36 Test loss: 8.210853, Test Accuracy:2481/5000 (49.62%)\n",
      "Epoch:37 Test loss: 7.939394, Test Accuracy:2600/5000 (52.0%)\n",
      "Epoch:38 Test loss: 8.211798, Test Accuracy:2535/5000 (50.7%)\n",
      "Epoch:39 Test loss: 8.458221, Test Accuracy:2541/5000 (50.82%)\n",
      "Epoch:40 Test loss: 8.602225, Test Accuracy:2554/5000 (51.08%)\n",
      "Epoch:41 Test loss: 9.293218, Test Accuracy:2419/5000 (48.38%)\n",
      "Epoch:42 Test loss: 8.931772, Test Accuracy:2498/5000 (49.96%)\n",
      "Epoch:43 Test loss: 8.516003, Test Accuracy:2629/5000 (52.58%)\n",
      "Epoch:44 Test loss: 8.577629, Test Accuracy:2625/5000 (52.5%)\n",
      "Epoch:45 Test loss: 9.179088, Test Accuracy:2448/5000 (48.96%)\n",
      "Epoch:46 Test loss: 9.054672, Test Accuracy:2575/5000 (51.5%)\n",
      "Epoch:47 Test loss: 9.184848, Test Accuracy:2521/5000 (50.42%)\n",
      "Epoch:48 Test loss: 8.650192, Test Accuracy:2620/5000 (52.4%)\n",
      "Epoch:49 Test loss: 9.090583, Test Accuracy:2516/5000 (50.32%)\n",
      "Epoch:50 Test loss: 9.290483, Test Accuracy:2531/5000 (50.62%)\n",
      "Epoch:51 Test loss: 8.637160, Test Accuracy:2645/5000 (52.9%)\n",
      "Epoch:52 Test loss: 8.706846, Test Accuracy:2601/5000 (52.02%)\n",
      "Epoch:53 Test loss: 9.263853, Test Accuracy:2467/5000 (49.34%)\n",
      "Epoch:54 Test loss: 9.200940, Test Accuracy:2466/5000 (49.32%)\n",
      "Epoch:55 Test loss: 8.768352, Test Accuracy:2524/5000 (50.48%)\n",
      "Epoch:56 Test loss: 8.959234, Test Accuracy:2481/5000 (49.62%)\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 200\n",
    "TEST = True\n",
    "LAMBDA = 0.01\n",
    "\n",
    "# model = CIFAR10Model()\n",
    "# model = CIFAR10ModelTest()\n",
    "# model = resnet18(3, 10)\n",
    "# model = CIFAR10ResLayer()\n",
    "# model = CIFAR100Model()\n",
    "\n",
    "\n",
    "# model, criterion, optimizer = transfer_radam_resnet101(100)\n",
    "\n",
    "model = resnet101(3, 100)\n",
    "summary(model, (3, 32, 32), device=\"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# # optimizer = optim.Adam(model.parameters())\n",
    "# optimizer = RAdam(model.parameters())\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "accuracies = []\n",
    "train_losses = []\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    loss = train(model, train_loader, optimizer, criterion, device, epoch)\n",
    "    train_losses.append(loss)\n",
    "    if not TEST:\n",
    "        acc, loss = validate(model, validation_loader, criterion, device, epoch)\n",
    "        accuracies.append(acc)\n",
    "        losses.append(loss)\n",
    "    else:\n",
    "        acc, loss = test(model, validation_loader, criterion, device, epoch, classes, 5)\n",
    "        accuracies.append(acc)\n",
    "        losses.append(loss)\n",
    "plt.figure(1)\n",
    "accuracy_plot(accuracies, TEST)\n",
    "plt.figure(2)\n",
    "losses_plot(train_losses, losses, TEST)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(accuracies, TEST)\n",
    "plt.savefig(\"accuracies151.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_plot(train_losses, losses, TEST)\n",
    "plt.savefig(\"losses151.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
