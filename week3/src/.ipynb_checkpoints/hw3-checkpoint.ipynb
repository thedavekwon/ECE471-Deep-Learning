{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(path, transform):\n",
    "    images = [transform(io.imread(os.path.join(path, file))) for file in os.listdir(path)]\n",
    "    labels = [int(file[-5]) for file in os.listdir(path)]\n",
    "    with open(path+\".p\", \"wb\") as f:\n",
    "        pickle.dump({\"images\":images, \"labels\":labels}, f)\n",
    "\n",
    "def load(path):\n",
    "    with open(path+\".p\", \"rb\") as f:\n",
    "        tmp = pickle.load(f)\n",
    "    return tmp[\"images\"], tmp[\"labels\"]\n",
    "\n",
    "transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"dataset/MNIST/train\", transform)\n",
    "save(\"dataset/MNIST/test\", transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(MNISTDataset).__init__()\n",
    "        self.images, self.labels = load(path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "def load_data():\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "    train_set = MNISTDataset(path=\"dataset/MNIST/train\")\n",
    "    train_set, vali_set = data.random_split(train_set, (54000, 6000))\n",
    "    test_set = MNISTDataset(path=\"dataset/MNIST/test\")\n",
    "    \n",
    "    print(len(train_set), len(vali_set), len(test_set))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=4096,\n",
    "                                               shuffle=True, num_workers=2)\n",
    "    vali_loader = torch.utils.data.DataLoader(vali_set, batch_size=4096,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=4096,\n",
    "                                              shuffle=False, num_workers=2)\n",
    "    return train_loader, vali_loader, test_loader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader, vali_loader, test_loader = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, 5, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(144, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.drop(F.relu(self.conv1(x))))\n",
    "        x = self.pool(self.drop(F.relu(self.conv2(x))))\n",
    "        x = self.pool(self.drop(F.relu(self.conv3(x))))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.drop(F.relu(self.fc1(x)))\n",
    "        x = self.drop(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        return functools.reduce(lambda a, b: a * b, x.size()[1:])\n",
    "\n",
    "class EfficientModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3)\n",
    "        self.bm1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 2, 3)\n",
    "        self.conv3 = nn.Conv2d(2, 4, 3)\n",
    "        self.bm3 = nn.BatchNorm2d(4)\n",
    "        self.conv4 = nn.Conv2d(4, 1, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4, 4)\n",
    "        self.fc2 = nn.Linear(4, 10)\n",
    "        # self.fc3 = nn.Linear(9, 10)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.avgPool = nn.AvgPool2d(2)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.drop(self.bm1(F.relu(self.conv1(x)))))\n",
    "        # x = self.pool(self.drop(self.bm2(F.relu(self.conv2(x)))))\n",
    "        x = self.drop(F.relu(self.conv2(x)))\n",
    "        x = self.pool(self.drop(self.bm3(F.relu(self.conv3(x)))))\n",
    "        x = self.pool(self.drop(F.elu(self.conv4(x))))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        return functools.reduce(lambda a, b: a * b, x.size()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LAMBDA = 0.005\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCH = 50\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, vali_loader, test_loader = load_data()\n",
    "\n",
    "#model = Model()\n",
    "model = EfficientModel()\n",
    "model.to(device)\n",
    "\n",
    "summary(model, (1, 28, 28))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    correct = 0\n",
    "    for X, y in train_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        regularization_loss = 0.0\n",
    "        for param in model.parameters():\n",
    "            regularization_loss += torch.norm(param)\n",
    "        loss = criterion(outputs, y) + LAMBDA * regularization_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "    train_percentage = round(correct / len(train_loader.dataset)*100, 2)\n",
    "    print(f\"Train Accuracy:{correct}/{len(train_loader.dataset)} ({train_percentage}%)\")\n",
    "    \n",
    "    vali_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in vali_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(X)\n",
    "            vali_loss += criterion(outputs, y).item()\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    vali_loss /= len(vali_loader.dataset)\n",
    "    vali_percentage = round(correct / len(vali_loader.dataset)*100, 2)\n",
    "    print(f\"Average Validation loss: {vali_loss:0.6f}, Validation Accuracy:{correct}/{len(vali_loader.dataset)} ({vali_percentage}%)\")\n",
    "\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(X)\n",
    "            test_loss += criterion(outputs, y).item()\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_percentage = round(correct / len(test_loader.dataset)*100, 2)\n",
    "\n",
    "\n",
    "    print(f\"Average Test loss: {test_loss:0.6f}, Test Accuracy:{correct}/{len(test_loader.dataset)} ({test_percentage}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
